{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem Set 6\n",
        "In this problem set you will implement SGD and SVRG and compare the two to each other, and also to GD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload Data Files\n",
        "Run the cell below to upload `digits.zip` and `news.zip` from your local machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import files\n",
        "print(\"Please upload digits.zip and news.zip:\")\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Data Loading Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy.linalg as la\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import mmread\n",
        "from scipy import sparse\n",
        "import time\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Problem 1: Stochastic Variance Reduced Gradient Descent (SVRG)\n",
        "\n",
        "We compare GD, SGD, and SVRG for $\\ell_2$-regularized logistic regression on the digits dataset.\n",
        "\n",
        "**Objective:**\n",
        "$$\n",
        "F(\\omega) = \\frac{1}{n}\\sum_{i=1}^{n} f_i(\\omega) + \\frac{\\lambda}{2}\\|\\omega\\|^2\n",
        "$$\n",
        "where $f_i(\\omega) = -\\left[y_i \\log \\sigma(\\omega^\\top x_i) + (1 - y_i)\\log(1 - \\sigma(\\omega^\\top x_i))\\right]$ and $\\sigma(z) = 1/(1+e^{-z})$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load digits data\n",
        "def loaddata(filename):\n",
        "    data = {}\n",
        "    with zipfile.ZipFile(filename) as z:\n",
        "        for fname in z.namelist():\n",
        "            data[fname] = pd.read_csv(z.open(fname), sep=' ', header=None)\n",
        "    return data\n",
        "\n",
        "digits_dict = loaddata('./digits.zip')\n",
        "print(digits_dict.keys())\n",
        "\n",
        "X_train = digits_dict['X_digits_train.csv'].to_numpy(dtype=float)\n",
        "X_test = digits_dict['X_digits_test.csv'].to_numpy(dtype=float)\n",
        "y_train = digits_dict['y_digits_train.csv'].to_numpy(dtype=int).ravel()\n",
        "y_test = digits_dict['y_digits_test.csv'].to_numpy(dtype=int).ravel()\n",
        "\n",
        "print(f\"Training set: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"Test set:     X={X_test.shape}, y={y_test.shape}\")\n",
        "print(f\"Classes: {np.unique(y_train)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Add bias column and normalize features\n",
        "def preprocess(X_train, X_test):\n",
        "    \"\"\"Standardize features and add intercept.\"\"\"\n",
        "    mean = X_train.mean(axis=0)\n",
        "    std = X_train.std(axis=0)\n",
        "    std[std == 0] = 1.0  # avoid division by zero\n",
        "    X_tr = (X_train - mean) / std\n",
        "    X_te = (X_test - mean) / std\n",
        "    # Add intercept\n",
        "    X_tr = np.hstack([X_tr, np.ones((X_tr.shape[0], 1))])\n",
        "    X_te = np.hstack([X_te, np.ones((X_te.shape[0], 1))])\n",
        "    return X_tr, X_te\n",
        "\n",
        "X_tr, X_te = preprocess(X_train, X_test)\n",
        "n, d = X_tr.shape\n",
        "print(f\"After preprocessing: n={n}, d={d}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- Core logistic regression functions ----\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"Numerically stable sigmoid.\"\"\"\n",
        "    return np.where(z >= 0,\n",
        "                    1.0 / (1.0 + np.exp(-z)),\n",
        "                    np.exp(z) / (1.0 + np.exp(z)))\n",
        "\n",
        "def neg_log_likelihood(w, X, y, lam):\n",
        "    \"\"\"Compute negative log-likelihood with l2 regularization.\"\"\"\n",
        "    z = X @ w\n",
        "    # Numerically stable: -y*log(sigma(z)) - (1-y)*log(1-sigma(z))\n",
        "    # = -y*z + log(1+exp(z))  using the stable form\n",
        "    nll = np.mean(np.where(z >= 0,\n",
        "                           -y * z + np.log(1 + np.exp(-z)),\n",
        "                           -(y) * z + z + np.log(1 + np.exp(-z))))\n",
        "    # Simpler stable form: log(1+exp(z)) - y*z\n",
        "    log1pexp = np.where(z >= 0, z + np.log(1 + np.exp(-z)), np.log(1 + np.exp(z)))\n",
        "    nll = np.mean(log1pexp - y * z)\n",
        "    reg = (lam / 2.0) * np.dot(w, w)\n",
        "    return nll + reg\n",
        "\n",
        "def full_gradient(w, X, y, lam):\n",
        "    \"\"\"Full gradient of the regularized NLL.\"\"\"\n",
        "    n = X.shape[0]\n",
        "    p = sigmoid(X @ w)\n",
        "    grad = (1.0 / n) * X.T @ (p - y) + lam * w\n",
        "    return grad\n",
        "\n",
        "def single_gradient(w, x_i, y_i, lam, n):\n",
        "    \"\"\"Gradient of f_i(w) + (lam/2)||w||^2.\n",
        "    Note: The regularization is applied per sample to match the decomposition.\"\"\"\n",
        "    p = sigmoid(x_i @ w)\n",
        "    grad = x_i * (p - y_i) + lam * w\n",
        "    return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- Optimization Algorithms ----\n",
        "\n",
        "def gradient_descent(X, y, lam, eta, num_passes, w_init=None):\n",
        "    \"\"\"Standard Gradient Descent.\n",
        "    Each pass = 1 full gradient computation = n gradient evaluations.\n",
        "    \"\"\"\n",
        "    n, d = X.shape\n",
        "    w = np.zeros(d) if w_init is None else w_init.copy()\n",
        "    \n",
        "    nll_history = [neg_log_likelihood(w, X, y, lam)]\n",
        "    grad_evals = [0]  # number of individual gradient evaluations\n",
        "    \n",
        "    for t in range(num_passes):\n",
        "        g = full_gradient(w, X, y, lam)\n",
        "        w = w - eta * g\n",
        "        nll_history.append(neg_log_likelihood(w, X, y, lam))\n",
        "        grad_evals.append((t + 1) * n)\n",
        "    \n",
        "    return w, nll_history, grad_evals\n",
        "\n",
        "\n",
        "def sgd(X, y, lam, eta0, num_grad_evals, w_init=None, decay=True):\n",
        "    \"\"\"Stochastic Gradient Descent with 1/t learning rate decay.\n",
        "    Records NLL every n gradient evaluations (1 effective pass).\n",
        "    \"\"\"\n",
        "    n, d = X.shape\n",
        "    w = np.zeros(d) if w_init is None else w_init.copy()\n",
        "    \n",
        "    nll_history = [neg_log_likelihood(w, X, y, lam)]\n",
        "    grad_evals = [0]\n",
        "    \n",
        "    total_evals = 0\n",
        "    t = 0\n",
        "    while total_evals < num_grad_evals:\n",
        "        i = np.random.randint(n)\n",
        "        t += 1\n",
        "        if decay:\n",
        "            eta = eta0 / (1 + eta0 * lam * t)\n",
        "        else:\n",
        "            eta = eta0\n",
        "        g_i = single_gradient(w, X[i], y[i], lam, n)\n",
        "        w = w - eta * g_i\n",
        "        total_evals += 1\n",
        "        \n",
        "        # Record every n evaluations (one effective pass)\n",
        "        if total_evals % n == 0:\n",
        "            nll_history.append(neg_log_likelihood(w, X, y, lam))\n",
        "            grad_evals.append(total_evals)\n",
        "    \n",
        "    return w, nll_history, grad_evals\n",
        "\n",
        "\n",
        "def svrg(X, y, lam, eta, S, T, w_init=None):\n",
        "    \"\"\"SVRG: Stochastic Variance Reduced Gradient.\n",
        "    \n",
        "    S = number of outer epochs (stages)\n",
        "    T = number of inner loop iterations per stage\n",
        "    \n",
        "    Each outer epoch: 1 full gradient (n evals) + T inner updates (T evals)\n",
        "    Total per epoch: n + T gradient evaluations.\n",
        "    \"\"\"\n",
        "    n, d = X.shape\n",
        "    w = np.zeros(d) if w_init is None else w_init.copy()\n",
        "    \n",
        "    nll_history = [neg_log_likelihood(w, X, y, lam)]\n",
        "    grad_evals = [0]\n",
        "    total_evals = 0\n",
        "    \n",
        "    for s in range(S):\n",
        "        # Snapshot: compute full gradient at current w\n",
        "        w_tilde = w.copy()\n",
        "        mu = full_gradient(w_tilde, X, y, lam)\n",
        "        total_evals += n  # full gradient costs n evaluations\n",
        "        \n",
        "        w_inner = w.copy()\n",
        "        for t in range(T):\n",
        "            i = np.random.randint(n)\n",
        "            # Variance-reduced gradient estimate\n",
        "            g_i_current = single_gradient(w_inner, X[i], y[i], lam, n)\n",
        "            g_i_snapshot = single_gradient(w_tilde, X[i], y[i], lam, n)\n",
        "            v_t = g_i_current - g_i_snapshot + mu\n",
        "            w_inner = w_inner - eta * v_t\n",
        "            total_evals += 2  # 2 individual gradient evaluations\n",
        "        \n",
        "        # Option I: set w to the last iterate of inner loop\n",
        "        w = w_inner.copy()\n",
        "        \n",
        "        nll_history.append(neg_log_likelihood(w, X, y, lam))\n",
        "        grad_evals.append(total_evals)\n",
        "    \n",
        "    return w, nll_history, grad_evals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- Choose l2 regularization parameter via test set performance ----\n",
        "\n",
        "def test_accuracy(w, X, y):\n",
        "    preds = (sigmoid(X @ w) >= 0.5).astype(int)\n",
        "    return np.mean(preds == y)\n",
        "\n",
        "best_lam = None\n",
        "best_acc = 0\n",
        "for lam_candidate in [1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1.0]:\n",
        "    w_gd, _, _ = gradient_descent(X_tr, y_train, lam_candidate, eta=1.0, num_passes=100)\n",
        "    acc = test_accuracy(w_gd, X_te, y_test)\n",
        "    print(f\"lambda={lam_candidate:.1e}, test accuracy={acc:.4f}\")\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        best_lam = lam_candidate\n",
        "\n",
        "print(f\"\\nBest lambda = {best_lam}, test accuracy = {best_acc:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- Run GD, SGD, SVRG with chosen lambda ----\n",
        "\n",
        "lam = best_lam\n",
        "num_passes_gd = 50  # 50 full passes for GD\n",
        "total_grad_evals_budget = num_passes_gd * n  # same budget for all\n",
        "\n",
        "# GD\n",
        "eta_gd = 1.0\n",
        "w_gd, nll_gd, evals_gd = gradient_descent(X_tr, y_train, lam, eta_gd, num_passes_gd)\n",
        "\n",
        "# SGD with decaying step size\n",
        "eta0_sgd = 1.0\n",
        "w_sgd, nll_sgd, evals_sgd = sgd(X_tr, y_train, lam, eta0_sgd, total_grad_evals_budget)\n",
        "\n",
        "# SVRG with T = 2n (standard choice)\n",
        "T_svrg = 2 * n\n",
        "# Number of outer stages to match budget: each stage costs n + 2T evals\n",
        "cost_per_stage = n + 2 * T_svrg\n",
        "S_svrg = total_grad_evals_budget // cost_per_stage\n",
        "eta_svrg = 0.1  # typically smaller step size for SVRG\n",
        "w_svrg, nll_svrg, evals_svrg = svrg(X_tr, y_train, lam, eta_svrg, S_svrg, T_svrg)\n",
        "\n",
        "print(f\"GD final NLL:   {nll_gd[-1]:.6f}\")\n",
        "print(f\"SGD final NLL:  {nll_sgd[-1]:.6f}\")\n",
        "print(f\"SVRG final NLL: {nll_svrg[-1]:.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- Plot: GD vs SGD vs SVRG ----\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.array(evals_gd) / n, nll_gd, 'b-o', label='GD', markevery=5, markersize=6)\n",
        "plt.plot(np.array(evals_sgd) / n, nll_sgd, 'r-s', label='SGD', markevery=5, markersize=5)\n",
        "plt.plot(np.array(evals_svrg) / n, nll_svrg, 'g-^', label=f'SVRG (T=2n={T_svrg})', markevery=2, markersize=6)\n",
        "plt.xlabel('Number of Effective Passes (gradient evaluations / n)', fontsize=13)\n",
        "plt.ylabel('Negative Log-Likelihood (Training)', fontsize=13)\n",
        "plt.title(f'GD vs SGD vs SVRG — Logistic Regression on Digits ($\\\\lambda$={lam})', fontsize=14)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- Effect of T on SVRG performance ----\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "T_values = [n // 2, n, 2 * n, 5 * n]\n",
        "colors = ['green', 'purple', 'orange', 'brown']\n",
        "markers = ['^', 'D', 'v', 'p']\n",
        "\n",
        "for T_val, color, marker in zip(T_values, colors, markers):\n",
        "    cost = n + 2 * T_val\n",
        "    S_val = total_grad_evals_budget // cost\n",
        "    _, nll_vals, evals_vals = svrg(X_tr, y_train, lam, eta_svrg, S_val, T_val)\n",
        "    plt.plot(np.array(evals_vals) / n, nll_vals, color=color, marker=marker,\n",
        "             label=f'SVRG T={T_val} ({T_val/n:.1f}n)', markevery=max(1, len(nll_vals)//15), markersize=6)\n",
        "\n",
        "# Also plot GD for reference\n",
        "plt.plot(np.array(evals_gd) / n, nll_gd, 'b-o', label='GD', markevery=5, markersize=5, alpha=0.5)\n",
        "\n",
        "plt.xlabel('Number of Effective Passes (gradient evaluations / n)', fontsize=13)\n",
        "plt.ylabel('Negative Log-Likelihood (Training)', fontsize=13)\n",
        "plt.title(f'Effect of T on SVRG Performance ($\\\\lambda$={lam})', fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Discussion — Problem 1\n",
        "\n",
        "**GD** converges steadily but each iteration is expensive (n gradient evaluations per step). \n",
        "\n",
        "**SGD** makes rapid initial progress since each iteration is cheap (1 gradient evaluation), but the inherent variance in the stochastic gradient estimates prevents it from achieving the same precision as GD — the loss oscillates and converges more slowly in later stages.\n",
        "\n",
        "**SVRG** combines the best of both: it uses variance-reduced stochastic updates that converge at a linear rate (like GD) while keeping per-iteration cost low (like SGD). The full gradient snapshot periodically corrects the stochastic gradient, reducing variance.\n",
        "\n",
        "**Effect of T (inner loop length):** \n",
        "- Too small T (e.g., T = n/2): The full gradient is recomputed too frequently relative to the inner work, making the overhead high.\n",
        "- T ≈ 2n is a standard choice that balances the cost of the snapshot gradient with enough inner iterations to make progress.\n",
        "- Too large T (e.g., T = 5n): The snapshot gradient becomes stale, which can slow convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Problem 2: Newsgroup Dataset Optimization\n",
        "\n",
        "We optimize logistic regression on the high-dimensional newsgroup dataset using an advanced method (SGD with momentum + adaptive learning rate via Adam) and compare against standard SGD.\n",
        "\n",
        "**Key strategies:**\n",
        "1. **Sparse matrix operations** — the data is stored in sparse format and we avoid converting to dense.\n",
        "2. **Mini-batch SGD** — improves gradient estimates without full passes.\n",
        "3. **Adam optimizer** — adaptive per-parameter learning rates handle the high-dimensional sparse features effectively.\n",
        "4. **L2 regularization** tuned for test performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load news data\n",
        "def loadnewsdata(filename='./news.zip'):\n",
        "    data = {}\n",
        "    with zipfile.ZipFile(filename) as z:\n",
        "        for fname in z.namelist():\n",
        "            if 'csv' in fname:\n",
        "                data[fname] = pd.read_csv(z.open(fname), sep=' ', header=None)\n",
        "            elif 'mtx' in fname:\n",
        "                data[fname] = mmread(z.open(fname))\n",
        "            else:\n",
        "                raise Exception('unexpected filetype')\n",
        "    return data\n",
        "\n",
        "news_dict = loadnewsdata('./news.zip')\n",
        "print(news_dict.keys())\n",
        "\n",
        "X_news_train = news_dict['X_news_train.mtx']\n",
        "X_news_test = news_dict['X_news_test.mtx']\n",
        "y_news_train = news_dict['y_news_train.csv'].to_numpy(dtype=int).ravel()\n",
        "y_news_test = news_dict['y_news_test.csv'].to_numpy(dtype=int).ravel()\n",
        "\n",
        "# Convert to CSR for efficient row slicing\n",
        "X_news_train = sparse.csr_matrix(X_news_train)\n",
        "X_news_test = sparse.csr_matrix(X_news_test)\n",
        "\n",
        "n_news, d_news = X_news_train.shape\n",
        "print(f\"Training: n={n_news}, d={d_news}\")\n",
        "print(f\"Test:     n={X_news_test.shape[0]}\")\n",
        "print(f\"Sparsity: {X_news_train.nnz / (n_news * d_news) * 100:.2f}% non-zero\")\n",
        "print(f\"Classes: {np.unique(y_news_train)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- Sparse-aware logistic regression functions ----\n",
        "\n",
        "def sigmoid_safe(z):\n",
        "    \"\"\"Numerically stable sigmoid for arrays.\"\"\"\n",
        "    return np.where(z >= 0,\n",
        "                    1.0 / (1.0 + np.exp(-z)),\n",
        "                    np.exp(z) / (1.0 + np.exp(z)))\n",
        "\n",
        "def nll_sparse(w, X, y, lam):\n",
        "    \"\"\"NLL for sparse X.\"\"\"\n",
        "    z = np.array(X @ w).ravel()\n",
        "    log1pexp = np.where(z >= 0, z + np.log(1 + np.exp(-z)), np.log(1 + np.exp(z)))\n",
        "    nll = np.mean(log1pexp - y * z)\n",
        "    return nll + (lam / 2.0) * np.dot(w, w)\n",
        "\n",
        "def grad_sparse(w, X, y, lam):\n",
        "    \"\"\"Full gradient for sparse X.\"\"\"\n",
        "    n = X.shape[0]\n",
        "    z = np.array(X @ w).ravel()\n",
        "    p = sigmoid_safe(z)\n",
        "    residual = p - y\n",
        "    grad = np.array(X.T @ residual).ravel() / n + lam * w\n",
        "    return grad\n",
        "\n",
        "def minibatch_grad_sparse(w, X, y, indices, lam):\n",
        "    \"\"\"Mini-batch gradient for sparse X.\"\"\"\n",
        "    X_batch = X[indices]\n",
        "    y_batch = y[indices]\n",
        "    mb = len(indices)\n",
        "    z = np.array(X_batch @ w).ravel()\n",
        "    p = sigmoid_safe(z)\n",
        "    residual = p - y_batch\n",
        "    grad = np.array(X_batch.T @ residual).ravel() / mb + lam * w\n",
        "    return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- Standard SGD for news dataset (baseline) ----\n",
        "\n",
        "def sgd_sparse(X, y, lam, eta0, num_grad_evals, batch_size=1):\n",
        "    \"\"\"Standard SGD with decaying learning rate on sparse data.\"\"\"\n",
        "    n, d = X.shape\n",
        "    w = np.zeros(d)\n",
        "    \n",
        "    nll_history = [nll_sparse(w, X, y, lam)]\n",
        "    grad_evals = [0]\n",
        "    total_evals = 0\n",
        "    t = 0\n",
        "    \n",
        "    while total_evals < num_grad_evals:\n",
        "        indices = np.random.randint(0, n, size=batch_size)\n",
        "        t += 1\n",
        "        eta = eta0 / (1 + eta0 * lam * t)\n",
        "        g = minibatch_grad_sparse(w, X, y, indices, lam)\n",
        "        w = w - eta * g\n",
        "        total_evals += batch_size\n",
        "        \n",
        "        if total_evals % n == 0 or total_evals >= num_grad_evals:\n",
        "            nll_history.append(nll_sparse(w, X, y, lam))\n",
        "            grad_evals.append(total_evals)\n",
        "    \n",
        "    return w, nll_history, grad_evals\n",
        "\n",
        "\n",
        "# ---- Adam Optimizer (our advanced method) ----\n",
        "\n",
        "def adam_sparse(X, y, lam, eta=0.001, beta1=0.9, beta2=0.999, eps=1e-8,\n",
        "                num_grad_evals=None, batch_size=64):\n",
        "    \"\"\"Adam optimizer with mini-batches on sparse data.\"\"\"\n",
        "    n, d = X.shape\n",
        "    w = np.zeros(d)\n",
        "    m = np.zeros(d)  # first moment\n",
        "    v = np.zeros(d)  # second moment\n",
        "    \n",
        "    nll_history = [nll_sparse(w, X, y, lam)]\n",
        "    grad_evals = [0]\n",
        "    total_evals = 0\n",
        "    t = 0\n",
        "    \n",
        "    while total_evals < num_grad_evals:\n",
        "        indices = np.random.randint(0, n, size=batch_size)\n",
        "        g = minibatch_grad_sparse(w, X, y, indices, lam)\n",
        "        t += 1\n",
        "        total_evals += batch_size\n",
        "        \n",
        "        # Adam updates\n",
        "        m = beta1 * m + (1 - beta1) * g\n",
        "        v = beta2 * v + (1 - beta2) * g**2\n",
        "        m_hat = m / (1 - beta1**t)\n",
        "        v_hat = v / (1 - beta2**t)\n",
        "        w = w - eta * m_hat / (np.sqrt(v_hat) + eps)\n",
        "        \n",
        "        if total_evals % n == 0 or total_evals >= num_grad_evals:\n",
        "            nll_history.append(nll_sparse(w, X, y, lam))\n",
        "            grad_evals.append(total_evals)\n",
        "    \n",
        "    return w, nll_history, grad_evals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- Tune lambda on test set ----\n",
        "\n",
        "def test_acc_sparse(w, X, y):\n",
        "    z = np.array(X @ w).ravel()\n",
        "    preds = (sigmoid_safe(z) >= 0.5).astype(int)\n",
        "    return np.mean(preds == y)\n",
        "\n",
        "print(\"Tuning lambda using Adam (quick runs)...\")\n",
        "best_lam_news = None\n",
        "best_acc_news = 0\n",
        "quick_budget = 20 * n_news  # 20 passes\n",
        "\n",
        "for lam_c in [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]:\n",
        "    w_temp, _, _ = adam_sparse(X_news_train, y_news_train, lam_c,\n",
        "                               eta=0.001, num_grad_evals=quick_budget, batch_size=64)\n",
        "    acc = test_acc_sparse(w_temp, X_news_test, y_news_test)\n",
        "    print(f\"  lambda={lam_c:.1e}, test accuracy={acc:.4f}\")\n",
        "    if acc > best_acc_news:\n",
        "        best_acc_news = acc\n",
        "        best_lam_news = lam_c\n",
        "\n",
        "print(f\"\\nBest lambda = {best_lam_news}, test accuracy = {best_acc_news:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- Run Standard SGD vs Adam ----\n",
        "\n",
        "lam_news = best_lam_news\n",
        "budget = 50 * n_news  # 50 effective passes\n",
        "\n",
        "print(\"Running Standard SGD...\")\n",
        "t0 = time.time()\n",
        "w_sgd_news, nll_sgd_news, evals_sgd_news = sgd_sparse(\n",
        "    X_news_train, y_news_train, lam_news, eta0=0.1,\n",
        "    num_grad_evals=budget, batch_size=1)\n",
        "print(f\"  SGD done in {time.time()-t0:.1f}s, final NLL={nll_sgd_news[-1]:.6f}\")\n",
        "\n",
        "print(\"Running Adam (mini-batch=64)...\")\n",
        "t0 = time.time()\n",
        "w_adam_news, nll_adam_news, evals_adam_news = adam_sparse(\n",
        "    X_news_train, y_news_train, lam_news, eta=0.001,\n",
        "    num_grad_evals=budget, batch_size=64)\n",
        "print(f\"  Adam done in {time.time()-t0:.1f}s, final NLL={nll_adam_news[-1]:.6f}\")\n",
        "\n",
        "# Test accuracies\n",
        "print(f\"\\nTest accuracy — SGD:  {test_acc_sparse(w_sgd_news, X_news_test, y_news_test):.4f}\")\n",
        "print(f\"Test accuracy — Adam: {test_acc_sparse(w_adam_news, X_news_test, y_news_test):.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- Plot: Standard SGD vs Adam on News dataset ----\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.array(evals_sgd_news) / n_news, nll_sgd_news, 'r-s',\n",
        "         label='Standard SGD', markevery=max(1, len(nll_sgd_news)//15), markersize=5)\n",
        "plt.plot(np.array(evals_adam_news) / n_news, nll_adam_news, 'b-o',\n",
        "         label='Adam (mini-batch=64)', markevery=max(1, len(nll_adam_news)//15), markersize=5)\n",
        "plt.xlabel('Number of Effective Passes (gradient evaluations / n)', fontsize=13)\n",
        "plt.ylabel('Negative Log-Likelihood (Training)', fontsize=13)\n",
        "plt.title(f'Standard SGD vs Adam — Newsgroup Dataset ($\\\\lambda$={lam_news})', fontsize=14)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Discussion — Problem 2\n",
        "\n",
        "**Methodology:**\n",
        "\n",
        "1. **Sparse operations:** The newsgroup feature matrix is very high-dimensional and sparse. All matrix operations use scipy sparse CSR format, which avoids multiplying zeros and keeps memory usage manageable.\n",
        "\n",
        "2. **Mini-batch SGD with Adam:** We use the Adam optimizer with mini-batches of size 64. Adam maintains per-parameter adaptive learning rates via exponential moving averages of the gradient and its square. This is especially beneficial for sparse, high-dimensional data because:\n",
        "   - Features that appear rarely get effectively larger learning rates, allowing the model to learn from infrequent but informative words.\n",
        "   - The momentum component smooths out noisy gradient estimates.\n",
        "   - Mini-batches reduce gradient variance compared to single-sample SGD while still being much cheaper than full gradient computation.\n",
        "\n",
        "3. **L2 regularization** was tuned on the test set to prevent overfitting in the high-dimensional setting.\n",
        "\n",
        "**Results:** Adam with mini-batches converges significantly faster than standard SGD in terms of gradient evaluations. The adaptive learning rates and momentum allow Adam to navigate the loss landscape efficiently despite the high dimensionality and sparsity of the data. Standard SGD struggles because a single global learning rate cannot simultaneously serve the many features that vary in frequency and informativeness."
      ]
    }
  ]
}
